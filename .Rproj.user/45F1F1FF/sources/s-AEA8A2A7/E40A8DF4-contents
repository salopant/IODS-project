---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Week 4 results

##Topics

This week I learned 

Analysis: Clustering and classification i.e. unsupervised ML

- k - means
- classification of a new observation
- discriminant analysis i.e. "what makes the difference(s) between these groups (clusters)?"
- distance, dissimilarity, similarity measures 

Data: library MASS, data Boston
Description: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html

## Material (Emma Kämäräinen & Tuomo Nieminen):

### Videos

Classification: Linear Discriminant Analysis

https://player.vimeo.com/video/203184662

Clustering: distance measures and k-means

https://player.vimeo.com/video/203198932

### Slides

The presentation slides related to the videos above are here:

https://tuomonieminen.github.io/Helsinki-Open-Data-Science/#/41

### More info
Corrplot: https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html

## Analysis

###Upload data (Boston)

```{r}
#Antti Salopuro
#23.11.2018 
#This file is the result of the week 4 of course Introduction to Open Data Science 2018
#data source: MASS/Boston

#access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)
summary(Boston)

# plot matrix of the variables
pairs(Boston)
```

### Description

Illustrate with some nice graphs (library PerformanceAnalysis)
```{r}
library(PerformanceAnalytics)
chart.Correlation(Boston)

```

### Data features with some anaysis

#### crim

per capita crime rate by town.

```{r}
hist(Boston$crim,freq = F)
```

Seems that the crime rate has big variation, highest value, 88.97 suggests that in some region in Boston there really is a huge amount of crimes identified. Still, in most suburbs (89%) the rate stays below 10. 

#### zn

proportion of residential land zoned for lots over 25,000 sq.ft.

```{r}
hist(Boston$zn,freq = F)
```
Most of the lots are relatively small, this is natural as when a big lot is split into smaller pieces, it always produces two or more new lots which increases the frequency of small lots more than it decreses the number of bigger ones.

#### indus

proportion of non-retail business acres per town.

This feature varies between 0 and 25, and there seems to be quite a few suburs where the business is solely retail but also some suburbs have more these industrial businesses. 

#### chas

Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).


#### nox

nitrogen oxides concentration (parts per 10 million).

#### rm

average number of rooms per dwelling.

Almost like normally distributed variable with average of 6.2. 

#### age

proportion of owner-occupied units built prior to 1940.

#### dis

weighted mean of distances to five Boston employment centres.

#### rad

index of accessibility to radial highways.

#### tax

full-value property-tax rate per \$10,000.

#### ptratio

pupil-teacher ratio by town.

#### black

1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

#### lstat

lower status of the population (percent).

#### medv

median value of owner-occupied homes in \$1000s.

### Additional notes of features

Interesting notes about correlation graph: the crime rate seems to be extremely high only in some suburbs that are near to the city center. Also high crime rate happens only in suburbs with higher proportion of old houses (age) These are the same suburbs where also property taxes are high and smaller lots. Apparently this means that in the central areas the crime rates may be high.

In city center the lots are small, taxes higher and also the pollution (nox) level is higher.

The industrial companies locate themselves rather quite close to the city center.

Nitrogen oxid concentration nox is an interesting feature, especially when looked together with outher features. Seems that closer to city center the nox value increases, also correlates the proportion of industrial companies.

In city center, the number of rooms per dwelling can be practically anything, when getting further from the center, the number of rooms has less variance staying closer to the mean value 6.2.  

#### Correlation plot

```{r}
library(corrplot)
library(dplyr)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits=2)

# print the correlation matrix
print(cor_matrix)

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)

```


### Scale data 

Data standardization scales all explanatory variables such that their mean value and standard deviation are the same. The mean value become zero (0) and the standard deviation becomes one (1). This is required here since we want to apply discriminant analysis, which assumes that variables are normally distributed and that their variance is same. 

Scaling & Transfer into a dataframe:
```{r}
# MASS and Boston dataset are available

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```


### Crime rate categories

Create a categorical variable of crime rate with four categories: low, med_low, med_high, high

```{r}
# MASS, Boston and boston_scaled are available
library(dplyr)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low","med_low","med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

### Create train (80%) and test (20%) data 

```{r}
# boston_scaled is available

# number of rows in the Boston dataset 
n <- dim(boston_scaled)[1]

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```

### LDA model fit

Fit LDA on the train set. crime rate is the target and all other variables are predictors

```{r}
# MASS and train are available

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch = classes)

lda.arrows(lda.fit, myscale = 2)
```


### Predict test data crime rates

Save the real crime rates of the test data in variable correct_classes and then make prediction
```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

#predict 
# lda.fit, correct_classes and test are available
```

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# crosstabulate the results
(confusion <- table(correct = correct_classes, predicted = lda.pred$class))

```

From the confusion matrix we can see that 65% of the predictions are correctly matched with the correct category. In the class "high" the match is 100%. The class low is almost always predicted either into the low or med_low category. 

### K-means algorithm

To explore the Boston data with k-means algorithm, we need to reload & normalize the data.
```{r}
data(Boston)
# MASS and Boston dataset are available

# center and standardize variables
boston_scaled2 <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled2)

# class of the boston_scaled object
class(boston_scaled2)

# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled2)

```

#### Distances: euclidean

```{r}

# load MASS and Boston
library(MASS)
data('Boston')

# euclidean distance matrix
dist_eu <- dist(Boston, method = "euclidean")
```

#### Distances: manhattan

```{r}
# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method = "manhattan")

# look at the summary of the distances
summary(dist_man)
```

#### Find optimal number of centers:

```{r}
# MASS, ggplot2 and Boston dataset are available
library(ggplot2)
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```
Optimal number of centers seems to be 2 as the function value drops permanently after 2.

```{r}
# k-means clustering
km <-kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

```

The data seems to be well separable in two clusters with many pairs of variables. 

### Bonus task. 4 clusters

```{r}
# k-means clustering
kmBonus <-kmeans(Boston, centers = 4)

# plot the Boston dataset with clusters
pairs(Boston, col = kmBonus$cluster)

```

