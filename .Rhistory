g2 <- ggplot(my_data, aes(x = high_use, y = absences, col = romantic))
# define the plot as a boxplot and draw it
g2 + geom_boxplot() + ylab("absences") + ggtitle("Alcohol consumption and absences from school for students in/out of relationships")
# initialise a plot of high_use and absences
g2 <- ggplot(my_data, aes(x = high_use, y = absences, col = romantic))
# define the plot as a boxplot and draw it
g2 + geom_boxplot() + ylab("absences") + ggtitle("Alcohol consumption and absences from school for students in/out of relationships")
m <- glm(high_use ~ failures + absences + romantic + Pstatus, data = my_data, family = "binomial")
# print out a summary of the model
summary(m)
# print out the coefficients of the model
coef(m)
# compute odds ratios (OR)
OR <- coef(m) %>% exp
# compute confidence intervals (CI)
CI <- confint(m) %>% exp
# print out the odds ratios with their confidence intervals
cbind(OR, CI)
m <- glm(high_use ~ failures + absences, data = my_data, family = "binomial")
# print out a summary of the model
summary(m)
# compute odds ratios (OR)
OR <- coef(m) %>% exp
# compute confidence intervals (CI)
CI <- confint(m) %>% exp
# print out the odds ratios with their confidence intervals
cbind(OR, CI)
# predict() the probability of high_use
probabilities <- predict(m, type = "response")
# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability = probabilities)
# use the probabilities to make a prediction of high_use
alc <- mutate(alc, prediction = probability > 0.5)
# see the last ten original classes, predicted probabilities, and class predictions
select(alc, failures, absences, high_use, probability, prediction) %>% tail(10)
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction)
# predict() the probability of high_use
probabilities <- predict(m, type = "response")
# add the predicted probabilities to 'alc'
my_data <- mutate(my_data, probability = probabilities)
# use the probabilities to make a prediction of high_use
my_data <- mutate(my:data, prediction = probability > 0.5)
# see the last ten original classes, predicted probabilities, and class predictions
select(my_data, failures, absences, high_use, probability, prediction) %>% tail(10)
# tabulate the target variable versus the predictions
table(high_use = my_data$high_use, prediction = my_data$prediction)
# predict() the probability of high_use
probabilities <- predict(m, type = "response")
# add the predicted probabilities to 'my_data'
my_data <- mutate(my_data, probability = probabilities)
# use the probabilities to make a prediction of high_use
my_data <- mutate(my_data, prediction = probability > 0.5)
# see the last ten original classes, predicted probabilities, and class predictions
select(my_data, failures, absences, high_use, probability, prediction) %>% tail(10)
# tabulate the target variable versus the predictions
table(high_use = my_data$high_use, prediction = my_data$prediction)
# initialize a plot of 'high_use' versus 'probability' in 'alc'
g <- ggplot(my_data, aes(x = probability, y = high_use))
# define the geom as points and draw the plot
g + geom_point(aes(col = prediction))
# tabulate the target variable versus the predictions
table(high_use = my_data$high_use, prediction = my_data$prediction)%>% prop.table()%>%addmargins()
# access the MASS package
library(MASS)
# load the data
data("Boston")
# explore the dataset
str(Boston)
summary(Boston)
# plot matrix of the variables
pairs(Boston)
str(Boston)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round()
# print the correlation matrix
print(cor_matrix)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
#Antti Salopuro
#23.11.2018
#This file is the result of the week 4 of course Introduction to Open Data Science 2018
#data source: MASS/Boston
#access the MASS package
library(MASS)
# load the data
data("Boston")
# explore the dataset
str(Boston)
summary(Boston)
# plot matrix of the variables
pairs(Boston)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round()
# print the correlation matrix
print(cor_matrix)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round()
# print the correlation matrix
print(cor_matrix)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits=2)
# print the correlation matrix
print(cor_matrix)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston)# %>% round(digits=2)
# print the correlation matrix
print(cor_matrix)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
library(corrplot)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston)# %>% round(digits=2)
# print the correlation matrix
print(cor_matrix)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
library(corrplot)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston)# %>% round(digits=2)
# print the correlation matrix
print(cor_matrix)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
install.packages("corrplot")
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
chart.Correlation(Boston,col=Boston$medv)
library(PerformanceAnalytics)
chart.Correlation(Boston)
library(PerformanceAnalytics)
chart.Correlation(Boston,col=Boston$medv)
library(PerformanceAnalytics)
chart.Correlation(Boston[,1:6],col=Boston$medv)
library(PerformanceAnalytics)
chart.Correlation(Boston[,1:6])
library(PerformanceAnalytics)
chart.Correlation(Boston[,1:5])
library(PerformanceAnalytics)
chart.Correlation(Boston)
hist(Boston$crim)
hist(Boston$crim,freq = F)
hist(Boston$crim,freq = F)
sum(Boston$crim < 10)/length(Boston)
hist(Boston$crim,freq = F)
sum(Boston$crim < 20)/length(Boston)
hist(Boston$crim,freq = F)
sum(Boston$crim > 20)/length(Boston)
hist(Boston$crim,freq = T)
sum(Boston$crim > 20)/length(Boston)
hist(Boston$crim,freq = T)
length(Boston)
hist(Boston$crim,freq = T)
size(Boston)
hist(Boston$crim,freq = T)
dim(Boston)
hist(Boston$crim,freq = T)
sum(Boston$crim > 20)/dim(Boston)[1]
hist(Boston$crim,freq = T)
sum(Boston$crim <= 10)/dim(Boston)[1]
hist(Boston$crim,freq = F)
sum(Boston$crim <= 10)/dim(Boston)[1]
library(PerformanceAnalytics)
chart.Correlation(Boston)
hist(Boston$zn,freq = F)
library(PerformanceAnalytics)
chart.Correlation(Boston)
# MASS and train are available
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 2)
# MASS and train are available
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
# MASS and train are available
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
# boston_scaled is available
# number of rows in the Boston dataset
n <- dim(boston_scaled)[1]
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
(train <- boston_scaled[ind,])
# create test set
test <- boston_scaled[-ind,]
# MASS and Boston dataset are available
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# MASS and Boston dataset are available
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
(boston_scaled <- as.data.frame(boston_scaled))
# MASS, Boston and boston_scaled are available
library(dplyr)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low","med_low","med_high", "high"))
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# boston_scaled is available
# number of rows in the Boston dataset
n <- dim(boston_scaled)[1]
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set
test <- boston_scaled[-ind,]
# MASS and train are available
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
#predict
# lda.fit, correct_classes and test are available
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# crosstabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
#predict
# lda.fit, correct_classes and test are available
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# crosstabulate the results
#table(correct = correct_classes, predicted = lda.pred$class)
correct_classes
# Chunk 1
#Antti Salopuro
#23.11.2018
#This file is the result of the week 4 of course Introduction to Open Data Science 2018
#data source: MASS/Boston
#access the MASS package
library(MASS)
# load the data
data("Boston")
# explore the dataset
str(Boston)
summary(Boston)
# plot matrix of the variables
pairs(Boston)
# Chunk 2
library(PerformanceAnalytics)
chart.Correlation(Boston)
# Chunk 3
hist(Boston$crim,freq = F)
# Chunk 4
hist(Boston$zn,freq = F)
# Chunk 5
library(corrplot)
library(dplyr)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits=2)
# print the correlation matrix
print(cor_matrix)
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.8)
# Chunk 6
# MASS and Boston dataset are available
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# Chunk 7
# MASS, Boston and boston_scaled are available
library(dplyr)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low","med_low","med_high", "high"))
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# Chunk 8
# boston_scaled is available
# number of rows in the Boston dataset
n <- dim(boston_scaled)[1]
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set
test <- boston_scaled[-ind,]
# Chunk 9
# MASS and train are available
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
#predict
# lda.fit, correct_classes and test are available
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# crosstabulate the results
#table(correct = correct_classes, predicted = lda.pred$class)
correct_classes
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# crosstabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# crosstabulate the results
confusion <- table(correct = correct_classes, predicted = lda.pred$class)
sum(confusion)
sum(c(14,17,13,22))/sum(confusion)
14/sum(c(14,9,2))
# Boston dataset is available
# k-means clustering
km <-kmeans(Boston, centers = 2)
# plot the Boston dataset with clusters
pairs(Boston[6:10], col = km$cluster)
# Boston dataset is available
# k-means clustering
km <-kmeans(Boston, centers = 2)
# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
# Boston dataset is available
# k-means clustering
km <-kmeans(Boston, centers = 3)
# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
# Boston dataset is available
# k-means clustering
km <-kmeans(Boston, centers = 4)
# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
# MASS, ggplot2 and Boston dataset are available
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# MASS, ggplot2 and Boston dataset are available
library(ggplot2)
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# k-means clustering
km <-kmeans(Boston, centers = 2)
# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
# MASS, ggplot2 and Boston dataset are available
library(ggplot2)
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
# visualize the results
plot(x = 1:k_max, y = twcss, geom = 'line')
# MASS, ggplot2 and Boston dataset are available
library(ggplot2)
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
# visualize the results
line(x = 1:k_max, y = twcss)
# MASS, ggplot2 and Boston dataset are available
library(ggplot2)
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# k-means clustering
km <-kmeans(Boston, centers = 2)
# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
# k-means clustering
kmBonus <-kmeans(Boston, centers = 4)
# plot the Boston dataset with clusters
pairs(Boston, col = kmBonus$cluster)
source('C:/Users/antti/asalo/Opinnot/Intro to open data science/IODS-project/create_human.R', echo=TRUE)
source('C:/Users/antti/asalo/Opinnot/Intro to open data science/IODS-project/create_human.R', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/antti/asalo/Opinnot/Intro to open data science/IODS-project/create_human.R', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/antti/asalo/Opinnot/Intro to open data science/IODS-project/create_human.R', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/antti/asalo/Opinnot/Intro to open data science/IODS-project/create_human.R', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/antti/asalo/Opinnot/Intro to open data science/IODS-project/create_human.R', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/antti/asalo/Opinnot/Intro to open data science/IODS-project/create_human.R', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/antti/asalo/Opinnot/Intro to open data science/IODS-project/create_human.R', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/antti/asalo/Opinnot/Intro to open data science/IODS-project/create_human.R', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/antti/asalo/Opinnot/Intro to open data science/IODS-project/create_human.R', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/antti/asalo/Opinnot/Intro to open data science/IODS-project/create_human.R', encoding = 'UTF-8', echo=TRUE)
